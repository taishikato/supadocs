---
title: Chat Pipeline
description: Understand how Supadocs assembles retrieval-augmented answers
---

# Chat Pipeline

Every conversation relies on document chunks stored in Supabase to provide grounded answers with citations.

## Request Flow

1. The web client calls `/api/chat` with the user prompt and prior conversation.
2. The server loads the embedding model specified by `EMBEDDING_MODEL` and generates a query vector.
3. Supabase RPC `match_document_chunks` returns the most relevant passages from pgvector.
4. A chat model defined by `OPENAI_CHAT_MODEL` receives the prompt, retrieved context, and system guardrails.
5. The streaming response is piped back to the UI via the Vercel AI SDK.

## Configuration

- Change `OPENAI_CHAT_MODEL` and `EMBEDDING_MODEL` in `.env.local` to switch between providers.
- Configure rate limits or alternative providers inside the upcoming [`packages/agents-tools`](/docs/agent-runtime-preview) module.
- Learn how the embeddings stay fresh via [Reindex Architecture](/docs/reindex-architecture).

## Debugging Tips

- Inspect the Supabase SQL function `match_document_chunks` when relevance scores look off.
- Turn on verbose logging in the Edge Function to trace chunk creation per document.
- Use Supabase Studio vector previews to confirm that chunks include the expected markdown content.
